import requests, re, json, csv
from bs4 import BeautifulSoup

confirmed_CSV_URL = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv'
deaths_CSV_URL = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv'
recovered_CSV_URL = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv'

confirmed_total_data = []
deaths_total_data = []
recovered_total_data = []

korea_data = []

with requests.Session() as s:
    download = s.get(confirmed_CSV_URL)
    decoded_content = download.content.decode('utf-8')
    cr = csv.reader(decoded_content.splitlines(), delimiter=',')
    my_list = list(cr)
    for row in my_list:
        confirmed_total_data.append(row)

with requests.Session() as s:
    download = s.get(deaths_CSV_URL)
    decoded_content = download.content.decode('utf-8')
    cr = csv.reader(decoded_content.splitlines(), delimiter=',')
    my_list = list(cr)
    for row in my_list:
        deaths_total_data.append(row)

with requests.Session() as s:
    download = s.get(recovered_CSV_URL)
    decoded_content = download.content.decode('utf-8')
    cr = csv.reader(decoded_content.splitlines(), delimiter=',')
    my_list = list(cr)
    for row in my_list:
        recovered_total_data.append(row)

index = confirmed_total_data[0].index('2/1/20') #14
korea_confirmed_date = confirmed_total_data[0][index:]
korea_confirmed_date = [ i[:-3] for i in korea_confirmed_date ]

for i in confirmed_total_data:
    if i[1] == 'Korea, South':
        korea_confirmed = i[index:]

# korea_data = list(zip(korea_confirmed_date, korea_confirmed))

index = deaths_total_data[0].index('2/1/20') #14

korea_deaths_date = deaths_total_data[0][index:]
korea_deaths_date = [ i[:-3] for i in korea_deaths_date ]

for i in deaths_total_data:
    if i[1] == 'Korea, South':
        # print(i)
        korea_deaths = i[index:]

index = deaths_total_data[0].index('2/1/20') #14

korea_recovered_date = recovered_total_data[0][index:]
korea_recovered_date = [ i[:-3] for i in korea_recovered_date ]

for i in recovered_total_data:
    if i[1] == 'Korea, South':
        # print(i)
        korea_recovered = i[index:]


korea_data = list(zip(korea_deaths_date, korea_confirmed, korea_deaths, korea_recovered))
result = []
for i in range(len(korea_data)):
    if i == 0:
        result.append([
                    korea_data[i][0],
                    int(korea_data[i][1]),
                    1,
                    int(korea_data[i][2]),
                    int(korea_data[i][3])
                    ])
    else:
        result.append([
                    korea_data[i][0],
                    int(korea_data[i][1]),
                    int(korea_data[i][1]) - int(korea_data[i-1][1]),
                    int(korea_data[i][2]),
                    int(korea_data[i][3])
                    ])

with open("./data/koreaRegionalCumulativeData.js", "w", encoding='UTF-8-sig') as json_file:
    json.dump(result, json_file, ensure_ascii=False, indent=4)

data = ''
with open("./data/koreaRegionalCumulativeData.js", "r", encoding='UTF-8-sig') as f:
    while True:
        line = f.readline()
        if not line: break
        data += line
data = '//Auto-generated by crawlKoreaRegionalCumulativeData.py\nvar koreaRegionalCumulativeData = ' + data + ';'

with open("./data/koreaRegionalCumulativeData.js", "w", encoding='UTF-8-sig') as f_write:
    f_write.write(data)

print("############### 완료!! ###############")
print("#####################################")
